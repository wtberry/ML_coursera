{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization & Weights folding/unfolding\n",
    "\n",
    "### On this notebook, we'll cover tips for NN optimization\n",
    "1. Random Initialization\n",
    "2. Weights folding/unfolding\n",
    "\n",
    "Weights are usually 2D matrix in fully connected neural nets, but since scipy.minimize.optimize is used for\n",
    "this project, and it only accepts 1D matrix of parameters, neural net's weight matrix needs to be folded / unfolded.\n",
    "\n",
    "\n",
    "**For NumPy documentation...**\n",
    "- [numpy.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape)\n",
    "- [numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html)\n",
    "- [numpy.ndarray.size](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.size.html)\n",
    "- [numpy.ndarray.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)\n",
    "\n",
    "**Let's start with Random Initialization!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Initialization\n",
    "\n",
    "- Initialization of $\\Theta$ is necessary for optimizaiton algorithm to work. But what if we initialize it as matrix of 0s..???\n",
    "> Each $\\theta$ value would end up having same value, as a result, will not converge!!\n",
    "\n",
    "- So... we initialize them to a random value in $[-\\epsilon, \\epsilon]$ \n",
    "Mathematically.....\n",
    "$$ -\\epsilon\\le\\Theta_{ij}^{(l)}\\le\\epsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial theta1: \n",
      " [[-0.36509614  0.19920317  0.38754691 ...,  0.45924069  0.11615955\n",
      "  -0.35184345]\n",
      " [-0.46771081 -0.23989066  0.37161541 ...,  0.0685328  -0.37220195\n",
      "   0.08807854]\n",
      " [ 0.39716566  0.45892626 -0.08816415 ...,  0.09367224  0.28341768\n",
      "   0.3219025 ]\n",
      " ..., \n",
      " [-0.04134174 -0.21449344  0.45503016 ..., -0.29866822  0.43778739\n",
      "   0.27134899]\n",
      " [-0.28086922 -0.32664497  0.23147162 ..., -0.44852144 -0.19954163\n",
      "   0.20961565]\n",
      " [-0.24701615  0.34807752  0.38773301 ..., -0.0452624   0.29700791\n",
      "   0.06417084]]\n",
      "\n",
      "theta1: Max, Min:  0.499995723955 -0.499992764652\n",
      "Theta1, shape:  (100, 785)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## params\n",
    "eps = .5\n",
    "n = 784\n",
    "hidden_layer_size = 100\n",
    "num_labels = 10\n",
    "\n",
    "r_theta1 = np.random.random((hidden_layer_size, n+1))*2*eps-eps\n",
    "r_theta2 = np.random.random((num_labels,\n",
    "                            hidden_layer_size+1))*2*eps-eps\n",
    "\n",
    "print('initial theta1: \\n', r_theta1)\n",
    "print(\"\\ntheta1: Max, Min: \", r_theta1.max(), r_theta1.min())\n",
    "print(\"Theta1, shape: \", r_theta1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that `r_theta1` is in range of..\n",
    "\n",
    "$$-\\epsilon < \\Theta_{r1} < \\epsilon$$, \n",
    "\n",
    "where $\\epsilon = 0.5$ here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Weights folding\n",
    "\n",
    "**Here, we'll go over a method called 'folding/unfolding.'**\n",
    "\n",
    "Weight folding is a process to 'flatten' matrix, and concatinate $\\Theta$ matrices into one flat matrix (or should be called vector maybe??) so that we can feed it into optimization algorithm.\n",
    "\n",
    "First, $\\Theta_1$ has shape of `(100, 785)` as we saw in the last code.\n",
    "1. _Flattening_\n",
    "> Here, we use `np.reshape(args)` which change shape of matrix. Note that `np.size` returns the number of elements in the matrix. \n",
    "We can use it as one of the arguments for np.reshape, since 'flattening' meaning...\n",
    "    - (shape of the matrix) => (total number of elements x 1)\n",
    "\n",
    "2. Concatenating the $\\Theta$ s\n",
    "> After flattening all thetas, we will concatinate them into one vector using `np.concatenate()`. It takes tuple of the matrics as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of r_theta1 before folding...\n",
      " (100, 785)\n",
      "np.size returns the # of elements...\n",
      " 78500\n",
      "flattened theta1...\n",
      " [ 0.05302608  0.42610533 -0.19749085 ...,  0.26062377  0.37601568\n",
      " -0.45652982]\n",
      "\n",
      "flattened theta2...\n",
      " [-0.15014029 -0.31277123 -0.35601266 ...,  0.10094713  0.43902183\n",
      " -0.49759141]\n",
      "\n",
      "flattened theta1, shape...\n",
      " (78500,)\n",
      "flattened theta2, shape:\n",
      " (1010,)\n"
     ]
    }
   ],
   "source": [
    "print('Size of r_theta1 before folding...\\n', r_theta1.shape)\n",
    "print(\"np.size returns the # of elements...\\n\", r_theta1.size)\n",
    "\n",
    "# Let's flatten it!\n",
    "flattened_theta1 = r_theta1.reshape(r_theta1.size, order='F')\n",
    "print('flattened theta1...\\n', flattened_theta1)\n",
    "flattened_theta2 = r_theta2.reshape(r_theta2.size, order='F')\n",
    "print('\\nflattened theta2...\\n', flattened_theta2)\n",
    "\n",
    "print('\\nflattened theta1, shape...\\n', flattened_theta1.shape)\n",
    "print('flattened theta2, shape:\\n', flattened_theta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "concatenated theta\n",
      " [ 0.05302608  0.42610533 -0.19749085 ...,  0.10094713  0.43902183\n",
      " -0.49759141]\n",
      "\n",
      "concatenated theta, shape:\n",
      " (79510,)\n"
     ]
    }
   ],
   "source": [
    "## Let's concatenate them\n",
    "r_nn_params = np.concatenate((flattened_theta1, flattened_theta2))\n",
    "print('\\nconcatenated theta\\n', r_nn_params)\n",
    "print('\\nconcatenated theta, shape:\\n', r_nn_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, shapes of the flattened $\\Theta$ s are...\n",
    "- $\\Theta_1$: `(78500,)`\n",
    "- $\\Theta_2$: `(1010,)`\n",
    "\n",
    "Therefore, the concatenated theta's shape is the sum of these shapes, `(79510,)`\n",
    "\n",
    "This can be fed into optimization algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Unfolding\n",
    "\n",
    "We use `np.reshape()` method again, to unfold the `nn_param`. \n",
    "\n",
    "This method takes argument like `np.reshape(a, newshape, order)`, where\n",
    "- `a`: array like object (numpy array here)\n",
    "- `newshape`: new dimension to be reshaped\n",
    "- `order`: \n",
    "    > Read the elements of a using this index order, and place the elements into the reshaped array using this index order. _Numpy documentation_\n",
    "    \n",
    "We'll first index the appropriate portions of the nn_param vector to corresponding theta1 and 2. \n",
    "Then we'll reshape them into the original shape.\n",
    "Here, let's take a look at theta1 as our example.\n",
    "1. indexing:\n",
    "> Theta1's original shape is `(hidden_layer_size, n+1)`. Therefore, in `nn_param` vector, corresponding index for the theta1 should be at index 0 - (size of theta1), where size of theta1 is <br> hidden_layer_size $\\times$ n+1.\n",
    "\n",
    "2. reshaping:\n",
    "> We know the original shape of the theta1, so the newshape param is simply..<br> `(hidden_layer_size, n+1)`\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat theta1\n",
      " [ 0.05302608  0.42610533 -0.19749085 ...,  0.26062377  0.37601568\n",
      " -0.45652982]\n",
      "\n",
      "flat theta1 shape: \n",
      " (78500,)\n"
     ]
    }
   ],
   "source": [
    "## 1. indexing\n",
    "# t1 as indexed flat theta1\n",
    "t1 = r_nn_params[:hidden_layer_size * (n+1)]\n",
    "print('flat theta1\\n', t1)\n",
    "print('\\nflat theta1 shape: \\n', t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped theta1:\n",
      " [[ 0.05302608 -0.44979394  0.2860511  ..., -0.46321588  0.14193639\n",
      "  -0.414402  ]\n",
      " [ 0.42610533  0.49394322 -0.47261149 ..., -0.22663834  0.19509747\n",
      "   0.11949658]\n",
      " [-0.19749085  0.08226684 -0.45153108 ...,  0.39725172 -0.18171361\n",
      "  -0.29573581]\n",
      " ..., \n",
      " [ 0.22880737  0.10516532  0.03017331 ...,  0.38462516  0.42151461\n",
      "   0.26062377]\n",
      " [ 0.11692454 -0.20717831 -0.41355435 ...,  0.21901405  0.07787854\n",
      "   0.37601568]\n",
      " [ 0.26405228 -0.18460549 -0.18506799 ...,  0.46598531 -0.40163424\n",
      "  -0.45652982]]\n",
      "\n",
      "theta1 shape:  (100, 785)\n"
     ]
    }
   ],
   "source": [
    "## 2. Reshape it\n",
    "theta1 = np.reshape(t1, (hidden_layer_size, n+1), order='F')\n",
    "\n",
    "print('Reshaped theta1:\\n', theta1)\n",
    "print('\\ntheta1 shape: ', theta1.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
